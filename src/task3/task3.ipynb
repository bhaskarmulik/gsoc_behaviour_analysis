{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using spacy for NER\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>url</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>num_upvotes</th>\n",
       "      <th>selftext</th>\n",
       "      <th>created</th>\n",
       "      <th>vader_title_sentiment</th>\n",
       "      <th>vader_selftext_sentiment</th>\n",
       "      <th>vader_sentiment</th>\n",
       "      <th>final_risk_score</th>\n",
       "      <th>risk_classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mom died 3 hour ago</td>\n",
       "      <td>2861</td>\n",
       "      <td>kbqsnq</td>\n",
       "      <td>SuicideWatch</td>\n",
       "      <td>https://www.reddit.com/r/SuicideWatch/comments...</td>\n",
       "      <td>161</td>\n",
       "      <td>2861</td>\n",
       "      <td>thought id ready day realized thats impossible...</td>\n",
       "      <td>2020-12-12 15:22:24</td>\n",
       "      <td>-0.5574</td>\n",
       "      <td>0.846</td>\n",
       "      <td>positive</td>\n",
       "      <td>3.34</td>\n",
       "      <td>Medium Risk</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 title  score      id     subreddit  \\\n",
       "0  mom died 3 hour ago   2861  kbqsnq  SuicideWatch   \n",
       "\n",
       "                                                 url  num_comments  \\\n",
       "0  https://www.reddit.com/r/SuicideWatch/comments...           161   \n",
       "\n",
       "   num_upvotes                                           selftext  \\\n",
       "0         2861  thought id ready day realized thats impossible...   \n",
       "\n",
       "               created  vader_title_sentiment  vader_selftext_sentiment  \\\n",
       "0  2020-12-12 15:22:24                -0.5574                     0.846   \n",
       "\n",
       "  vader_sentiment  final_risk_score risk_classification  \n",
       "0        positive              3.34         Medium Risk  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./../../data/final_reddit_data.csv\")\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Loaded 1186 posts\n",
      "Loading spaCy model...\n",
      "Extracting locations using spaCy...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1186/1186 [00:31<00:00, 37.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 117 location mentions in 80 posts\n",
      "Geocoding locations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [01:20<00:00,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully geocoded 117 out of 117 location mentions\n",
      "Creating Folium heatmap...\n",
      "Saved Folium heatmap to 'reddit_crisis_heatmap.html'\n",
      "Creating Plotly map visualization...\n",
      "Saved Plotly map to 'reddit_crisis_plotly_map.html'\n",
      "Generating top locations report...\n",
      "\n",
      "Top 5 Locations by Post Count:\n",
      "      location  post_count  avg_risk_score\n",
      "77          uk           7        4.318889\n",
      "20      europe           5        5.446000\n",
      "9   california           4        2.215000\n",
      "42      mexico           4        4.952500\n",
      "54          ny           3        7.190000\n",
      "\n",
      "Top 5 Locations by Average Risk Score (min 2 posts):\n",
      "   location  post_count  avg_risk_score\n",
      "32    japan           2          7.8250\n",
      "54       ny           3          7.1900\n",
      "20   europe           5          5.4460\n",
      "72    texas           2          5.4050\n",
      "42   mexico           4          4.9525\n",
      "\n",
      "Top 5 Locations by High Risk Post Count:\n",
      "     location  high_risk_count  post_count\n",
      "54         ny                2           3\n",
      "52  nova rock                2           1\n",
      "26    florida                2           2\n",
      "20     europe                2           5\n",
      "0         a&w                1           1\n",
      "\n",
      "Analysis complete! Output files:\n",
      "- reddit_crisis_heatmap.html (Folium heatmap)\n",
      "- reddit_crisis_plotly_map.html (Plotly interactive map)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import spacy\n",
    "import geocoder\n",
    "from collections import Counter\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Load the data\n",
    "def load_data(file_path):\n",
    "    print(\"Loading data...\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"Loaded {len(df)} posts\")\n",
    "    return df\n",
    "\n",
    "# Extract locations using spaCy's named entity recognition\n",
    "def extract_locations_spacy(df):\n",
    "    print(\"Loading spaCy model...\")\n",
    "    # Use a more accurate model for NER\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "    \n",
    "    print(\"Extracting locations using spaCy...\")\n",
    "    locations = []\n",
    "    \n",
    "    # Combine title and selftext for processing\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        combined_text = f\"{row['title']} {row['selftext']}\"\n",
    "        \n",
    "        # Process text with spaCy\n",
    "        doc = nlp(combined_text)\n",
    "        \n",
    "        # Extract location entities (GPE, LOC)\n",
    "        loc_entities = [ent.text for ent in doc.ents if ent.label_ in [\"GPE\", \"LOC\"]]\n",
    "        \n",
    "        if loc_entities:\n",
    "            # Associate locations with risk score\n",
    "            for loc in loc_entities:\n",
    "                locations.append({\n",
    "                    'post_id': row['id'],\n",
    "                    'location': loc,\n",
    "                    'risk_score': row['final_risk_score'],\n",
    "                    'risk_class': row['risk_classification']\n",
    "                })\n",
    "    \n",
    "    # Create DataFrame from extracted locations\n",
    "    loc_df = pd.DataFrame(locations)\n",
    "    print(f\"Found {len(loc_df)} location mentions in {len(set(loc_df['post_id']))} posts\")\n",
    "    return loc_df\n",
    "\n",
    "# Alternative location extraction using regex and gazetteer approach\n",
    "def extract_locations_regex(df):\n",
    "    print(\"Extracting locations using regex patterns...\")\n",
    "    \n",
    "    # List of common locations to check (countries, states, major cities)\n",
    "    common_locations = [\n",
    "        # Major US cities\n",
    "        'new york', 'los angeles', 'chicago', 'houston', 'phoenix', 'philadelphia',\n",
    "        'san antonio', 'san diego', 'dallas', 'austin', 'seattle', 'boston', 'miami',\n",
    "        'denver', 'atlanta', 'portland', 'san francisco', 'nashville', 'baltimore',\n",
    "        # US states\n",
    "        'california', 'texas', 'florida', 'new york state', 'pennsylvania', 'illinois', \n",
    "        'ohio', 'georgia', 'michigan', 'north carolina', 'new jersey',\n",
    "        # Countries\n",
    "        'usa', 'united states', 'america', 'canada', 'mexico', 'uk', 'united kingdom',\n",
    "        'england', 'australia', 'india', 'germany', 'france', 'japan', 'china',\n",
    "        # Regions\n",
    "        'europe', 'asia', 'africa', 'south america', 'middle east'\n",
    "    ]\n",
    "    \n",
    "    # Compile regex patterns for each location (word boundary to avoid partial matches)\n",
    "    patterns = {loc: re.compile(r'\\b' + re.escape(loc) + r'\\b', re.IGNORECASE) for loc in common_locations}\n",
    "    \n",
    "    locations = []\n",
    "    \n",
    "    # Process each post\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        combined_text = f\"{row['title']} {row['selftext']}\"\n",
    "        \n",
    "        # Check for location matches\n",
    "        for loc, pattern in patterns.items():\n",
    "            if pattern.search(combined_text):\n",
    "                locations.append({\n",
    "                    'post_id': row['id'],\n",
    "                    'location': loc,\n",
    "                    'risk_score': row['final_risk_score'],\n",
    "                    'risk_class': row['risk_classification']\n",
    "                })\n",
    "    \n",
    "    # Create DataFrame from extracted locations\n",
    "    loc_df = pd.DataFrame(locations)\n",
    "    print(f\"Found {len(loc_df)} location mentions in {len(set(loc_df['post_id']))} posts\")\n",
    "    return loc_df\n",
    "\n",
    "# Geocode locations to get coordinates\n",
    "def geocode_locations(loc_df):\n",
    "    print(\"Geocoding locations...\")\n",
    "    \n",
    "    # Group by location to avoid redundant geocoding\n",
    "    unique_locations = loc_df['location'].unique()\n",
    "    \n",
    "    # Dictionary to store geocoded results\n",
    "    geo_dict = {}\n",
    "    \n",
    "    for loc in tqdm(unique_locations):\n",
    "        try:\n",
    "            # Use geocoder to get coordinates\n",
    "            g = geocoder.arcgis(loc)\n",
    "            if g.ok:\n",
    "                geo_dict[loc] = {\n",
    "                    'lat': g.lat,\n",
    "                    'lng': g.lng,\n",
    "                    'address': g.address\n",
    "                }\n",
    "            else:\n",
    "                # Fallback for some common locations\n",
    "                if loc.lower() == 'uk' or loc.lower() == 'united kingdom':\n",
    "                    geo_dict[loc] = {'lat': 51.5074, 'lng': -0.1278, 'address': 'United Kingdom'}\n",
    "                elif loc.lower() == 'usa' or loc.lower() == 'united states' or loc.lower() == 'america':\n",
    "                    geo_dict[loc] = {'lat': 39.8283, 'lng': -98.5795, 'address': 'United States'}\n",
    "                elif loc.lower() == 'europe':\n",
    "                    geo_dict[loc] = {'lat': 48.8566, 'lng': 2.3522, 'address': 'Europe'}\n",
    "                else:\n",
    "                    print(f\"Could not geocode: {loc}\")\n",
    "            \n",
    "            # Add a small delay to avoid rate limiting\n",
    "            time.sleep(0.2)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error geocoding {loc}: {e}\")\n",
    "    \n",
    "    # Add coordinates to the location dataframe\n",
    "    geocoded_df = loc_df.copy()\n",
    "    geocoded_df['lat'] = geocoded_df['location'].map(lambda x: geo_dict.get(x, {}).get('lat'))\n",
    "    geocoded_df['lng'] = geocoded_df['location'].map(lambda x: geo_dict.get(x, {}).get('lng'))\n",
    "    geocoded_df['full_address'] = geocoded_df['location'].map(lambda x: geo_dict.get(x, {}).get('address'))\n",
    "    \n",
    "    # Remove rows with failed geocoding\n",
    "    geocoded_df = geocoded_df.dropna(subset=['lat', 'lng'])\n",
    "    \n",
    "    print(f\"Successfully geocoded {len(geocoded_df)} out of {len(loc_df)} location mentions\")\n",
    "    return geocoded_df\n",
    "\n",
    "# Create a folium heatmap\n",
    "def create_folium_heatmap(geocoded_df):\n",
    "    print(\"Creating Folium heatmap...\")\n",
    "    \n",
    "    # Create a map centered on the average coordinates\n",
    "    center_lat = geocoded_df['lat'].mean()\n",
    "    center_lng = geocoded_df['lng'].mean()\n",
    "    \n",
    "    m = folium.Map(location=[center_lat, center_lng], zoom_start=3)\n",
    "    \n",
    "    # Prepare data for heatmap\n",
    "    # Weight by risk score\n",
    "    heat_data = [[row['lat'], row['lng'], row['risk_score']] \n",
    "                 for _, row in geocoded_df.iterrows()]\n",
    "    \n",
    "    # Add heatmap layer\n",
    "    HeatMap(heat_data, radius=15, blur=10, gradient={0.4: 'blue', 0.65: 'lime', 0.8: 'orange', 1: 'red'}).add_to(m)\n",
    "    \n",
    "    # Add markers for top locations\n",
    "    top_locations = geocoded_df.groupby(['location', 'lat', 'lng', 'full_address'])['risk_score'].agg(['count', 'mean']).reset_index()\n",
    "    top_locations = top_locations.sort_values('count', ascending=False).head(5)\n",
    "    \n",
    "    for _, row in top_locations.iterrows():\n",
    "        folium.Marker(\n",
    "            location=[row['lat'], row['lng']],\n",
    "            popup=f\"<b>{row['location']}</b><br>Posts: {row['count']}<br>Avg Risk: {row['mean']:.2f}\",\n",
    "            icon=folium.Icon(color='darkred', icon='info-sign')\n",
    "        ).add_to(m)\n",
    "    \n",
    "    # Save the map\n",
    "    m.save('reddit_crisis_heatmap.html')\n",
    "    print(\"Saved Folium heatmap to 'reddit_crisis_heatmap.html'\")\n",
    "    return m, top_locations\n",
    "\n",
    "# Create Plotly visualization\n",
    "def create_plotly_map(geocoded_df):\n",
    "    print(\"Creating Plotly map visualization...\")\n",
    "    \n",
    "    # Group by location\n",
    "    location_summary = geocoded_df.groupby(['location', 'lat', 'lng']).agg(\n",
    "        count=pd.NamedAgg(column='post_id', aggfunc='nunique'),\n",
    "        avg_risk=pd.NamedAgg(column='risk_score', aggfunc='mean')\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Create risk categories for color coding (Low, Medium, High)\n",
    "    location_summary['risk_category'] = pd.cut(\n",
    "        location_summary['avg_risk'],\n",
    "        bins=[0, 3, 5, 10],\n",
    "        labels=['Low Risk', 'Medium Risk', 'High Risk']\n",
    "    )\n",
    "    \n",
    "    # Create figure\n",
    "    fig = px.scatter_geo(\n",
    "        location_summary,\n",
    "        lat='lat',\n",
    "        lon='lng',\n",
    "        color='risk_category',\n",
    "        size='count',\n",
    "        hover_name='location',\n",
    "        hover_data={\n",
    "            'lat': False,\n",
    "            'lng': False,\n",
    "            'count': True,\n",
    "            'avg_risk': ':.2f',\n",
    "            'risk_category': True\n",
    "        },\n",
    "        projection='natural earth',\n",
    "        title='Crisis Mentions by Location in Reddit Posts',\n",
    "        color_discrete_map={\n",
    "            'Low Risk': 'green',\n",
    "            'Medium Risk': 'orange',\n",
    "            'High Risk': 'red'\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        height=600,\n",
    "        geo=dict(\n",
    "            showland=True,\n",
    "            landcolor='rgb(217, 217, 217)',\n",
    "            countrycolor='rgb(255, 255, 255)',\n",
    "            coastlinecolor='rgb(255, 255, 255)',\n",
    "            showocean=True,\n",
    "            oceancolor='rgb(220, 230, 255)'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Save as HTML file\n",
    "    fig.write_html('reddit_crisis_plotly_map.html')\n",
    "    print(\"Saved Plotly map to 'reddit_crisis_plotly_map.html'\")\n",
    "    \n",
    "    return fig, location_summary\n",
    "\n",
    "# Generate top locations report\n",
    "def generate_top_locations_report(geocoded_df):\n",
    "    print(\"Generating top locations report...\")\n",
    "    \n",
    "    # Group by location and calculate statistics\n",
    "    location_stats = geocoded_df.groupby('location').agg(\n",
    "        post_count=pd.NamedAgg(column='post_id', aggfunc='nunique'),\n",
    "        avg_risk_score=pd.NamedAgg(column='risk_score', aggfunc='mean'),\n",
    "        high_risk_count=pd.NamedAgg(column='risk_class', aggfunc=lambda x: sum(x == 'High Risk'))\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Sort by post count\n",
    "    top_locations_by_count = location_stats.sort_values('post_count', ascending=False).head(10)\n",
    "    \n",
    "    # Sort by average risk score (considering only locations with at least 2 posts)\n",
    "    top_locations_by_risk = location_stats[location_stats['post_count'] >= 2].sort_values('avg_risk_score', ascending=False).head(10)\n",
    "    \n",
    "    # Sort by high risk posts\n",
    "    top_locations_by_high_risk = location_stats.sort_values('high_risk_count', ascending=False).head(10)\n",
    "    \n",
    "    print(\"\\nTop 5 Locations by Post Count:\")\n",
    "    print(top_locations_by_count[['location', 'post_count', 'avg_risk_score']].head(5))\n",
    "    \n",
    "    print(\"\\nTop 5 Locations by Average Risk Score (min 2 posts):\")\n",
    "    print(top_locations_by_risk[['location', 'post_count', 'avg_risk_score']].head(5))\n",
    "    \n",
    "    print(\"\\nTop 5 Locations by High Risk Post Count:\")\n",
    "    print(top_locations_by_high_risk[['location', 'high_risk_count', 'post_count']].head(5))\n",
    "    \n",
    "    return top_locations_by_count, top_locations_by_risk, top_locations_by_high_risk\n",
    "\n",
    "# Main function to run the complete analysis pipeline\n",
    "def main():\n",
    "    # Step 1: Load the data\n",
    "    data_file = './../../data/final_reddit_data.csv'\n",
    "    df = load_data(data_file)\n",
    "    \n",
    "    # Step 2: Extract locations (choose one method or combine both)\n",
    "    # Option 1: Using spaCy NER (more accurate but slower)\n",
    "    try:\n",
    "        loc_df = extract_locations_spacy(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Error with spaCy extraction: {e}\")\n",
    "        print(\"Falling back to regex extraction...\")\n",
    "        loc_df = extract_locations_regex(df)\n",
    "    \n",
    "    if len(loc_df) == 0:\n",
    "        print(\"No locations found with spaCy, trying regex approach...\")\n",
    "        loc_df = extract_locations_regex(df)\n",
    "    \n",
    "    # Step 3: Geocode the locations\n",
    "    geocoded_df = geocode_locations(loc_df)\n",
    "    \n",
    "    # Step 4: Create visualizations\n",
    "    _, top_folium_locations = create_folium_heatmap(geocoded_df)\n",
    "    _, location_summary = create_plotly_map(geocoded_df)\n",
    "    \n",
    "    # Step 5: Generate report on top locations\n",
    "    top_count, top_risk, top_high_risk = generate_top_locations_report(geocoded_df)\n",
    "    \n",
    "    print(\"\\nAnalysis complete! Output files:\")\n",
    "    print(\"- reddit_crisis_heatmap.html (Folium heatmap)\")\n",
    "    print(\"- reddit_crisis_plotly_map.html (Plotly interactive map)\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 locations with highest crisis discussions:\n",
      "  location  count\n",
      "0   phobia      6\n",
      "1    kinda      5\n",
      "2     meth      5\n",
      "3    china      5\n",
      "4  florida      4\n",
      "Heatmap saved to crisis_heatmap.html\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"600\"\n",
       "            src=\"crisis_heatmap.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f945f0e6ed0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "\n",
    "# Load dataset\n",
    "data_path = './../../data/final_reddit_data.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Initialize spaCy model for NER (place recognition)\n",
    "# en_core_web_trf is more accurate but larger; fallback to en_core_web_sm if needed\n",
    "try:\n",
    "    nlp = spacy.load('en_core_web_trf')\n",
    "except OSError:\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Function to extract GPE entities from text\n",
    "def extract_locations(text):\n",
    "    doc = nlp(str(text))\n",
    "    return [ent.text for ent in doc.ents if ent.label_ == 'GPE']\n",
    "\n",
    "# Apply extraction to posts\n",
    "df['locations'] = df['selftext'].apply(extract_locations)\n",
    "\n",
    "# Explode to one location per row\n",
    "df_exploded = df.explode('locations').dropna(subset=['locations'])\n",
    "\n",
    "# Count crisis-related posts per location\n",
    "# Assuming all posts are crisis-related; if not, filter by a crisis keyword column\n",
    "location_counts = df_exploded['locations'].value_counts().rename_axis('location').reset_index(name='count')\n",
    "\n",
    "# Initialize geocoder with rate limiter\n",
    "geolocator = Nominatim(user_agent='crisis_heatmap_app')\n",
    "geocode = RateLimiter(geolocator.geocode, min_delay_seconds=1)\n",
    "\n",
    "# Geocode unique locations\n",
    "location_counts['geocode'] = location_counts['location'].apply(lambda loc: geocode(loc))\n",
    "# Drop failed geocodes\n",
    "location_counts = location_counts.dropna(subset=['geocode'])\n",
    "# Extract lat/lon\n",
    "location_counts['latitude'] = location_counts['geocode'].apply(lambda x: x.latitude)\n",
    "location_counts['longitude'] = location_counts['geocode'].apply(lambda x: x.longitude)\n",
    "\n",
    "# Get top 5 locations\n",
    "top5 = location_counts.nlargest(5, 'count')\n",
    "print(\"Top 5 locations with highest crisis discussions:\")\n",
    "print(top5[['location', 'count']])\n",
    "\n",
    "# Create base Folium map\n",
    "# Center on the mean coordinates\n",
    "map_center = [location_counts['latitude'].mean(), location_counts['longitude'].mean()]\n",
    "folium_map = folium.Map(location=map_center, zoom_start=2)\n",
    "\n",
    "# Prepare heat data\n",
    "heat_data = list(zip(location_counts['latitude'], location_counts['longitude'], location_counts['count']))\n",
    "\n",
    "# Add heatmap layer\n",
    "HeatMap(heat_data, radius=15, max_zoom=10).add_to(folium_map)\n",
    "\n",
    "# Save map to HTML\n",
    "output_map = 'crisis_heatmap.html'\n",
    "folium_map.save(output_map)\n",
    "print(f\"Heatmap saved to {output_map}\")\n",
    "\n",
    "# Optionally, display within a Jupyter environment\n",
    "try:\n",
    "    from IPython.display import IFrame\n",
    "    display(IFrame(output_map, width=800, height=600))\n",
    "except ImportError:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gsoc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
