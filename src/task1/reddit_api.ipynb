{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tracemalloc import stop\n",
    "import praw\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import time\n",
    "import ahocorasick\n",
    "from logging import Logger, FileHandler, Formatter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "stop_words = set(stopwords.words('english'))\n",
    "wordnet = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "load_dotenv(dotenv_path=\"./../.env\")\n",
    "#Get the secret keys from the .env file\n",
    "CLIENT_ID = os.getenv(\"REDDIT_CLIENT_ID\")\n",
    "CLIENT_SECRET = os.getenv(\"REDDIT_SECRET_KEY\")\n",
    "USER_AGENT = os.getenv(\"REDDIT_USER_AGENT\")\n",
    "USERNAME = os.getenv(\"REDDIT_USERNAME\")\n",
    "PASSWORD = os.getenv(\"REDDIT_PASSWORD\")\n",
    "logger = Logger(\"RedditBot\") \n",
    "formatter = Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "handler = FileHandler(\"./../../data/reddit_log.log\")\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize Reddit API\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id=CLIENT_ID,\n",
    "    client_secret=CLIENT_SECRET,\n",
    "    user_agent=USER_AGENT,\n",
    "    username=USERNAME,\n",
    "    password=PASSWORD\n",
    ")\n",
    "\n",
    "logger.info(\"Reddit API initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pattern_matching(text, keywords):\n",
    "    # Build Aho-Corasick Trie\n",
    "    trie = ahocorasick.Automaton()\n",
    "    for idx, keyword in enumerate(keywords):\n",
    "        trie.add_word(keyword, (idx, keyword))\n",
    "    trie.make_automaton()\n",
    "\n",
    "    # Check for keyword matches\n",
    "    matches = list(trie.iter(text))\n",
    "    if matches:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "keywords = [\"depressed\", \"anxiety\", \"suicidal\", \"overwhelmed\", \"addiction help\", \"self-harm\", \"mental health\", \"panic attack\", \"mental breakdown\",\"intrusive thoughts\",\"exhausted emotionally\",\"burnout\",\"social anxiety\",\"imposter syndrome\",\"emotional numbness\",\"existential crisis\", \"depression\", \"drugs\", \"sober\", \"alcohol\", \"addiction\", \"substance abuse\", \"relapse\", \"withrawal\", \"alienated\", \"lonely\", \"isolated\", \"alone\"]\n",
    "\n",
    "logger.info(f\"Keywords loaded. There are {len(keywords)} keywords and they are : {keywords}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = text.replace(\"\\r\", \" \")\n",
    "    text = text.replace(\"\\t\", \" \")\n",
    "    #remove all characters that are not ascii\n",
    "    text = text.str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "    #remove urls\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
    "    #remove all stop words\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    #Remove short words\n",
    "    text = ' '.join([w for w in text.split() if len(w)>3])\n",
    "    #lemmatize the words\n",
    "    text = ' '.join([wordnet.lemmatize(word) for word in text.split()])\n",
    "    \n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a list of subreddits to scrape\n",
    "subreddits = ['SuicideWatch', 'Suicidalideations', 'suicidaltendencies', 'Suicidal_Comforters', 'depression', 'depression_help', 'SubstanceAbuseHelp', 'stopdrinking', 'Anxiety', 'Anxietyhelp', 'AnxietyDepression']\n",
    "\n",
    "def scrape_reddit(subreddits : list, logger) -> pd.DataFrame:\n",
    "    #Create a dictionary to store the data\n",
    "    data = list()\n",
    "    seen_titles = set()\n",
    "\n",
    "    for subreddit in subreddits:\n",
    "        subreddit = reddit.subreddit(subreddit)\n",
    "\n",
    "        try:\n",
    "            for submission in subreddit.top(limit=100, time_filter='all'):\n",
    "                text = submission.selftext.lower()\n",
    "                title = submission.title.lower()\n",
    "                if pattern_matching(text, keywords) and (title not in seen_titles):\n",
    "                    seen_titles.add(title)\n",
    "                    data.append({\n",
    "                        \"title\" : clean_text(title),\n",
    "                        \"score\" : submission.score,\n",
    "                        \"id\" : submission.id,\n",
    "                        \"subreddit\" : submission.subreddit,\n",
    "                        \"url\" : submission.url,\n",
    "                        \"num_comments\" : submission.num_comments,\n",
    "                        \"num_upvotes\" : submission.ups,\n",
    "                        \"selftext\" : clean_text(text),\n",
    "                        \"created\" : submission.created\n",
    "                    }\n",
    "                    )\n",
    "                else:\n",
    "                    logger.info(f\"No matches found or already seen title: {submission.title} |||| The title already seen status : {title not in seen_titles}\")\n",
    "            time.sleep(1)\n",
    "            for submission in subreddit.top(limit = 100, time_filter = 'month'):\n",
    "                text = submission.selftext.lower()\n",
    "                title = submission.title.lower()\n",
    "                if pattern_matching(text, keywords) and (title not in seen_titles):\n",
    "                    seen_titles.add(title)\n",
    "                    data.append({\n",
    "                        \"title\" : clean_text(title),\n",
    "                        \"score\" : submission.score,\n",
    "                        \"id\" : submission.id,\n",
    "                        \"subreddit\" : submission.subreddit,\n",
    "                        \"url\" : submission.url,\n",
    "                        \"num_comments\" : submission.num_comments,\n",
    "                        \"num_upvotes\" : submission.ups,\n",
    "                        \"selftext\" : clean_text(text),\n",
    "                        \"created\" : submission.created\n",
    "                    }\n",
    "                    )\n",
    "                else:\n",
    "                    logger.info(f\"No matches found or already seen title: {submission.title} |||| The title already seen status : {title not in seen_titles}\")\n",
    "            time.sleep(1)\n",
    "            for submission in subreddit.hot(limit = 100):\n",
    "                text = submission.selftext.lower()\n",
    "                title = submission.title.lower()\n",
    "                if pattern_matching(text, keywords) and (title not in seen_titles):\n",
    "                    seen_titles.add(title)\n",
    "                    data.append({\n",
    "                        \"title\" : clean_text(title),\n",
    "                        \"score\" : submission.score,\n",
    "                        \"id\" : submission.id,\n",
    "                        \"subreddit\" : submission.subreddit,\n",
    "                        \"url\" : submission.url,\n",
    "                        \"num_comments\" : submission.num_comments,\n",
    "                        \"num_upvotes\" : submission.ups,\n",
    "                        \"selftext\" : clean_text(text),\n",
    "                        \"created\" : submission.created\n",
    "                    }\n",
    "                    )\n",
    "                else:\n",
    "                    logger.info(f\"No matches found or already seen title: {submission.title} |||| The title already seen status : {title not in seen_titles}\")\n",
    "            time.sleep(1)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "    df = pd.DataFrame(data, columns=[\"title\", \"score\", \"id\", \"subreddit\", \"url\", \"num_comments\", \"num_upvotes\", \"selftext\", \"created\"])\n",
    "    df['created'] = pd.to_datetime(df['created'], unit='s')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to reddit_data.csv\n"
     ]
    }
   ],
   "source": [
    "df = scrape_reddit(subreddits, logger=logger)\n",
    "df.to_csv(\"./../../data/reddit_data.csv\", index=False)\n",
    "print(\"Data saved to reddit_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gsoc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
